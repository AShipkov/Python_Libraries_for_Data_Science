{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1\n",
    "Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?\n",
    "\n",
    "\n",
    "### micro\n",
    "показатель F1-Score — гармоническое среднее. Применяется, когда количества объектов разных классов кратно отличаются. Подсчитывает общее количество истинных срабатываний, false negatives и false positives.\n",
    "### macro \n",
    "показатель средний F1-Score. Плохо работает с несбалансированными классами (когда количество объектов одного класса кратно больше, чем другого).\n",
    "### weighted\n",
    "показатель macro, но с учетом дисбаланса классов (за счет весовых коэффициентов для каждого класса).\n",
    "\n",
    "## Задание 2\n",
    "В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?\n",
    "\n",
    "### xgboost (extreme gradient boosting)\n",
    "это модернизированный градиентный бустинг, с идеальной комбинацией оптимизации ПО и железа для получения точных релузьтатов за короткое время с минимальным использованием вычислительных ресурсов. Под капотом ансамбли методов деревьев, которые используют принцип бустинга слабых учеников.\n",
    "\n",
    "### lightgbm\n",
    "относится к классу ансамблевых алгоритмов машинного обучения, которые могут использоваться для задач классификации или регрессионного прогностического моделирования.\n",
    "Реализация вводит две ключевые идеи: GOSS и EFB.\n",
    "Градиентная односторонняя выборка (GOSS) является модификацией градиентного бустинга, который фокусирует внимание на тех учебных примерах, которые приводят к большему градиенту, в свою очередь, ускоряя обучение и уменьшая вычислительную сложность метода.\n",
    "С помощью GOSS исключается значительная доля экземпляров данных с небольшими градиентами и используется только остальные экземпляры для оценки прироста информации. Мы доказываем, что, поскольку экземпляры данных с большими градиентами играют более важную роль в вычислении информационного выигрыша, GOSS может получить довольно точную оценку информационного выигрыша с гораздо меньшим размером данных.\n",
    "Exclusive Feature Bundling (объединение взаимоисключающих признаков), или EFB, — это подход объединения разрежённых (в основном нулевых) взаимоисключающих признаков, таких как категориальные переменные входных данных, закодированные унитарным кодированием. Таким образом, это тип автоматического подбора признаков.\n",
    "Преимущества перед XGBoost\n",
    "использует алгоритм на основе гистограммы, то есть он объединяет непрерывные значения признаков в дискретные ячейки, которые ускоряют процедуру обучения.\n",
    "заменяет непрерывные значения на дискретные ячейки, что приводит к меньшему использованию памяти.\n",
    "он создает гораздо более сложные деревья, следуя подходу разделения листьев, а не поуровневому подходу, который является основным фактором в достижении более высокой точности. Однако иногда это может привести к переобучению, чего можно избежать, установив параметр max_depth.\n",
    "одинаково хорошо работает с большими наборами данных при значительном сокращении времени обучения по сравнению с XGBOOST.\n",
    "\n",
    "### catboost\n",
    "библиотека градиентного бустинга, созданная Яндексом. Использует небрежные (oblivious) деревья решений, чтобы вырастить сбалансированное дерево. Одни и те же функции используются для создания левых и правых разделений (split) на каждом уровне дерева. По сравнению с классическими деревьями, небрежные деревья более эффективны при реализации на процессоре и просты в обучении.\n",
    "Наиболее распространенными способами обработки категориальных данных в машинном обучении является one-hot кодирование и кодирование лейблов. CatBoost позволяет использовать категориальные признаки без необходимости их предварительно обрабатывать. При использовании CatBoost мы не должны пользоваться one-hot кодированием, поскольку это влияет на скорость обучения и на качество прогнозов. Вместо этого мы просто задаем категориальные признаки с помощью параметра cat_features. Преимущества использования CatBoost\n",
    "CatBoost позволяет проводить обучение на нескольких GPU.\n",
    "Библиотека позволяет получить отличные результаты с параметрами по умолчанию, что сокращает время, необходимое для настройки гиперпараметров.\n",
    "Обеспечивает повышенную точность за счет уменьшения переобучения.\n",
    "Возможность быстрого предсказания с применением модели CatBoost;\n",
    "Обученные модели CatBoost можно экспортировать в Core ML для вывода на устройстве (iOS).\n",
    "Умеет под капотом обрабатывать пропущенные значения.\n",
    "Может использоваться для регрессионных и классификационных задач.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
